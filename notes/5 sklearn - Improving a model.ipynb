{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34213cc7",
   "metadata": {},
   "source": [
    "## Improving a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aedc6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First predictions = baseline predictions.\n",
    "# First model = baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24828281",
   "metadata": {},
   "source": [
    "From a data prespective:\n",
    "* Could I collect more date? (generally, more date = better)\n",
    "* Could I improve the data? (more feature within a sample)\n",
    "\n",
    "From a model perspective:\n",
    "* Is there a better model I can use?\n",
    "* Could I improve the currect model?\n",
    "\n",
    "Parameter = model find these patterns in data\n",
    "Hyperparameters = settings on a model you can adjut to try to improve its \n",
    "ability to find patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f8b46",
   "metadata": {},
   "source": [
    "## Three ways to adjust hyperparameters:\n",
    "1. By hand;\n",
    "2. Randomly with RandomSearchCV;\n",
    "3. Exhaustively with GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5583328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard import for all projects\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b9aa92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d05f8b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ab4b1",
   "metadata": {},
   "source": [
    "## 1. Improving hyperparams by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11215786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the 3 sets : training, validation and test\n",
    "# Try and adjust: max_depth, max_features, min_sample_leaf,min_sample_split, n_estimators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0beff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation function if you want to do something more than once\n",
    "def evaluate_preds(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    Performs evaluation comparison on y_true labels vs. y_pred labels\n",
    "    on a classification model.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy(y_true, y_preds)\n",
    "    precision = precision_score(y_true, y_preds)\n",
    "    recall = recall_score(y_true, y_preds)\n",
    "    f1 = f1_score(y_true, y_preds)\n",
    "    metric_dict = {\"accuracy\": round(accuracy, 2),\n",
    "                   \"precision\": round(precision, 2),\n",
    "                   \"recall\": round(recall, 2),\n",
    "                   \"f1\": round(f1, 2)}\n",
    "    print(f\"Acc: {accuracy * 100:.2}%\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 score {f1:.2f}\")\n",
    "    \n",
    "    return metric_dict\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
